---
title: "Removal data analysis: Okaloosa darters"
author: "Roy Martin"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
    toc_depth: 5
    number_sections: true
    keep_html: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, echo = FALSE, warning=FALSE, message=FALSE}
library(readxl)
library(ggpubr)
library(ggplot2)
library(ggExtra)
library(gridExtra)
library(dplyr)
library(tidyverse)
library(tidyr)
library(bayesplot)
library(tidybayes)
library(rstan)

options(mc.cores = parallel::detectCores(logical = FALSE))
options( max.print = 1000 )

# stat: skew 
skew <- function(x) {
  xdev <- x - mean(x)
  n <- length(x)
  r <- sum(xdev^3) / sum(xdev^2)^1.5
  return(r * sqrt(n) * (1 - 1/n)^1.5)
}
```

# Import data
```{r import_data}
path <- "C:/Users/rmartin/OneDrive - Environmental Protection Agency (EPA)/Documents/CSS_Data/Dorazio_Darter_Removal.csv"

df_import <- path %>%
  read.csv() %>%
  as_tibble()

# create a removal data frame with nrows = n individuals and data for:
# site number
# removal session r in 1:R removals
df_removal <- df_import %>%
  select(Site.number, r1, r2, r3) %>% # only first three removals as in Dorazio 2005
  pivot_longer(cols = c(r1, r2, r3), names_to = "Pass") %>%
  uncount(value)


#save(df_cleaned, file = "C:/Users/rmartin/OneDrive - Environmental Protection Agency (EPA)/Documents/ICPMS_methods/ICPMS_7d_Expt/model_files/df_cleaned.rda")
```


# Single species Stan model
```{stan mocc_model_1, eval=FALSE, include=TRUE, output.var='mod1'}
data {
  int<lower = 1> nind; // number of individuals captured (across all site)
  int<lower = 1> nrem; // number of removals
  int<lower = 1> nsite; // number of sites
  int<lower = 1> site [nind]; // indicator for s in s=1,..., S sites
  //real L[nind]; // length covariate
  array[nind, nrem] int <lower = 0, upper = 1>  y; // removal capture history
}

parameters {
  real b0; // logit-scale intercept on p
  real b1; // logit scale effect of L on p
  real<lower = 0> sigma_p; // scale of site to site variation in p
  vector[nsite] gamma_std; // non-centered site effects
}

parameters {
  real b0; // logit-scale intercept on p
  real b1; // logit scale effect of L on p
  real<lower = 0> sigma_p; // scale of site to site variation in p
  vector[nsite] gamma_std; // non-centered site effects
}

transformed parameters {
  vector [nind] log_lik;
  real<lower = 0, upper = 1> p[nind];
  vector [nsite] gamma;
  vector[nind] pi0;
  vector[nind] pcap;
  vector[nrem] pie[nind];
  simplex[nrem] muc[nind];
  
  // ranefs for p
  gamma = gamma_std * sigma_p;
  
  for(i in 1:nind) {
    p[i] = inv_logit(b0 + b1 * L[i] + gamma[site[i]]); // probability of capture
    pi0[i] = (1 - p[i]) ^ nrem; // prob not captured across all removals
    pcap[i] = 1 - pi0[i]; // prob captured across all removals
    
    for(j in 1:nrem) {
      pie[i, j] = p[i] * (1 - p[i]) ^ (j - 1); // probability of removal on pass j
      muc[i, j] = pie[i, j] / pcap[i]; // multinomial probs for y|ncap
      }
    
    log_lik[i] = multinomial_lpmf(y[i, ] | muc[i, ]);
    }
}

model {
  // priors
  target += normal_lpdf(b0 | 0, 10);
  target += normal_lpdf(b1 | 0, 5);
  target += exponential_lpdf(sigma_p | 1);
  target += normal_lpdf(gamma_std | 0, 1);
  
  // sum log-likelihood
  target += sum(log_lik);
}

generated quantities {
  int <lower = 0> N_i[nind]; // neg binom estimate of N conditional on p for each individual caught
  real N_h[nind]; // Huggins estimate per individual (N_i = 1 / pcap_i)
  int <lower = 0> N_total_nb;
  real N_tot_huggins;
  
  for(i in 1:nind){
    N_h[i] = 1 / pcap[i];
    N_i[i] = 1 + neg_binomial_rng(1.0 - pcap[i], pcap[i]); // rnbinom(1, 1 * (1 - pcap), pcap)
    }
  N_total_nb = sum(N_i);
  N_tot_huggins = sum(N_h);
}
```

### Make a data list
For single species
```{r data_list}
y <- matrix(NA, nrow(df_removal), 3) # R = 3 removal passes

for(i in 1:nrow(y)){
  if(df_removal[i,]$Pass == "r1"){
    y[i, ] <- c(1, 0, 0)
    }
  else if(df_removal[i,]$Pass == "r2"){
           y[i, ] <- c(0, 1, 0)
           } 
  else {y[i, ] <- c(0, 0, 1)}
}



data1 <- list(y = y,
              nind = dim(y)[[1]],
              site = df_removal$Site.number, 
              nsite = length(unique(df_removal$Site.number)),
              nrem = dim(y)[[2]]
              )
```

### Fit
Fit the model via $rstan$ interface to $Stan$
```{r fit_mod1, eval=FALSE, include=TRUE}

fit1 <- sampling(
  object = mod1,
  data = data1,
  chains = 4,
  iter = 1000,
  cores = 4,
  thin = 1,
  seed = 1357#,
  #control = list(adapt_delta = 0.95, max_treedepth = 14)
  )

#save(fit1, file = "C:/Users/rmartin/OneDrive - Environmental Protection Agency (EPA)/Documents/eDNA_RARE/model_files/fit1.rda")
```

### Parameters summary
Tabular summary of parameters in the linear predictors.
```{r print_mod1, echo=TRUE}
#load("C:/Users/rmartin/OneDrive - Environmental Protection Agency (EPA)/Documents/eDNA_RARE/model_files/fit1.rda")

print(fit1, pars = c("beta", "sigma_p", "N", "lp__"), digits_summary = 2)
```

Extract the draws
```{r extract_mod1, echo=TRUE}
#load("C:/Users/rmartin/OneDrive - Environmental Protection Agency (EPA)/Documents/eDNA_RARE/model_files/fit1.rda")

draws1 <- extract(fit1)
```


Plot and tables summarizing N and p by site
```{r p_plots_mod1, fig.align="center", fig.asp=1.5, fig.width=8}
# Plot p by site
par(mfrow = c(5, 4))
for(s in 1:max(data1$site)){
  hist(plogis(draws1$beta + draws1$gamma[, s]), 
       freq = F,
       nclass = 40, 
       xlim = c(0, 1),
       main = paste0("Site ", s),
       ylab = "Density",
       xlab = expression(hat(p)))
  abline(v = plogis(mean(draws1$beta)), col = 'red', lwd = 3)
}
```

```{r p_table_mod1, fig.align="center", fig.asp=1.1, fig.width=6}
# Summarise p by site
p_table <- matrix(NA, nrow = max(data1$site), ncol = 4)

for(s in 1:max(data1$site)){
  p_table[s, 1] <- round(mean(plogis(draws1$beta + draws1$gamma[, s])), 2) # means
  p_table[s, 2:4] <- round(quantile(plogis(draws1$beta + draws1$gamma[, s]), probs = c(0.025, 0.5, 0.975)), 2)
}

p_table <- p_table %>%
  `colnames<-` (c("mean", "L95", "median", "U95")) %>%
  as_tibble() %>%
  mutate(site = seq(1, 20, 1)) %>%
  select(site, mean, median, L95, U95)

p_table %>%
  ggpubr::ggtexttable(rows = NULL) %>%
  tab_add_title(text = "Summary of p by site")
```


```{r N_plots_mod1, fig.align="center", fig.asp=1.5, fig.width=8}
# Plot N by site
par(mfrow = c(5, 4))

for(s in 1:max(data1$site)){
  hist(rowSums(draws1$N_i[, data1$site == s]), 
       nclass = max(rowSums(draws1$N_i[, data1$site == s]))/2, 
       freq = F,
       xlim = c(sum(data1$site == s) - 5, round(sum(data1$site == s) / plogis(mean(draws1$beta + draws1$gamma[, s])), 0)),
       main = paste0("Site ", s),
       ylab = "Density",
       xlab = expression(hat(N)))
  abline(v = sum(data1$site == s), col = 'red', lwd = 3)
  abline(v = rowSums(df_import[,5:9], na.rm = TRUE)[s], col = 'blue', lwd = 3)
}
```

# Negative binomial N
```{r N_table_mod1, fig.align="center", fig.asp=1.1, fig.width=6}
# Summarise p by site
N_table <- matrix(NA, nrow = max(data1$site), ncol = 4)

for(s in 1:(max(data1$site))){
  N_table[s, 1] <- round(mean(rowSums(draws1$N_i[, data1$site == s])), 1) # means
  N_table[s, 2:4 ] <- round(quantile(rowSums(draws1$N_i[, data1$site == s]), probs = c(0.025, 0.5, 0.975)), 0)
}

N_table <- N_table %>%
  `colnames<-` (c("mean", "L95", "median", "U95")) %>%
  as_tibble() %>%
  mutate(site = seq(1, 20, 1),
         X3 = rowSums(df_import[, 5:7]), # number removed in passes 1-3
         X5 = rowSums(df_import[,5:9], na.rm = TRUE)) %>% # number removed in all passes (1 - 5)
  select(site, X3, X5, mean, median, L95, U95)

N_table %>%
  ggpubr::ggtexttable(rows = NULL) %>%
  tab_add_title(text = "Summary of N by site") # "X5" is the total number caught in all removal passes (including passes 4-5)
```


# Huggins N
# Negative binomial N
```{r N_Huggins_table_mod1, fig.align="center", fig.asp=1.1, fig.width=6}
# Summarise p by site
N_table <- matrix(NA, nrow = max(data1$site), ncol = 4)

for(s in 1:(max(data1$site))){
  N_table[s, 1] <- round(mean(rowSums(draws1$N_h[, data1$site == s])), 1) # means
  N_table[s, 2:4 ] <- round(quantile(rowSums(draws1$N_h[, data1$site == s]), probs = c(0.025, 0.5, 0.975)), 0)
}

N_table <- N_table %>%
  `colnames<-` (c("mean", "L95", "median", "U95")) %>%
  as_tibble() %>%
  mutate(site = seq(1, 20, 1),
         X3 = rowSums(df_import[, 5:7]), # number removed in passes 1-3
         X5 = rowSums(df_import[,5:9], na.rm = TRUE)) %>% # number removed in all passes (1 - 5)
  select(site, X3, X5, mean, median, L95, U95)

N_table %>%
  ggpubr::ggtexttable(rows = NULL) %>%
  tab_add_title(text = "Summary of N by site") # "X5" is the total number caught in all removal passes (including passes 4-5)
```
